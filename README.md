# Proyecto para el ramo de Inteligencia Artificial

### MNIST es una base de datos de dƒ±gitos escritos a mano, que estan representados como una imagen de 28x28 pixeles en escala de grises donde cada pixel tiene una tonalidad que va desde 0 (blanco) a 1 (negro). La base de datos esta compuesta por 70.000 ejemplos (60.000 dispuestos para entrenamiento y 10.000 para pruebas). Estan disponibles dos archivos ‚ÄúTrain.csv‚Äù y ‚ÄúTest.csv‚Äù, cada archivo compuesto por 60.000 y 10.000 lƒ±neas, respectivamente. Cada lƒ±nea esta por 784 valores entre 0 y 255 que representan las distintas tonalidad de cada una de las 28 √ó 28 casillas, ademas de un valor 785 que indica el dƒ±gito dibujado (entre 0 y 9).

## Se pide:

- Resolver el problema planteado utilizando Redes de tipo Perceptron MultiCapa (MLP). Puede usar como
  modelo los ejercicios vistos en clases.

## Defina:

- La arquitectura de la red (cuantas capas y neuronas tendra).
- Cual sera la funcion de activacion de cada neurona.
- Cual sera la funcion de error para la red.
- Cuantas iteraciones de los datos de entrenamiento aplicara a su red.

## Debe:

- probar distintos modelos de red, de manera de encontrar la configuracion que minimize su error,
  evitando el overfitting.

## Pruebas:

1.  Para comenzar planteamos la primera soluci√≥n con una capa de entrada, una oculta y una √∫ltima de salida. Como la capa oculta posee 30 neuronas, decidimos hacer 3 iteraciones, dando como resultado una precisi√≥n de 89% en la primera vuelta y 95% en la √∫ltima.

2.  Luego probamos que pasaba con la precisi√≥n al momento de disminuir la cantidad de neuronas a solo 20 y que fueran solo 2 iteraciones, d√°ndonos como resultado un peor rendimiento en la iteraci√≥n final con un porcentaje del 93%

3.  Para aumentar ese porcentaje probamos aumentando las iteraciones a 5 y nos dio un porcentaje de precisi√≥n de 95% al igual que la primera vez.

4.  As√≠ que decidimos mantener la cantidad de neuronas e iteraciones, pero agregando dos neuronas m√°s a la prueba, d√°ndonos como resultado un porcentaje igual del 95% que la vez anterior, por lo tanto, agregar capas no mejoro el porcentaje de precisi√≥n.

5.  As√≠ que por ahora mantenemos las iteraciones, pero disminuimos la cantidad de capas a solo 1 pero con 50 neuronas, d√°ndonos un porcentaje de 97%. Entonces nos planteamos la pregunta. ¬øEs m√°s importante la cantidad de neuronas que capas? As√≠ que decidimos hacer m√°s pruebas para averiguarlo.

6.  Aumentamos la cantidad de neuronas de la √∫nica capa oculta y mejoro el porcentaje a 98% de precisi√≥n, pero pareciera haber un peque√±o overfitting.

7.  Para verificar el overfitting decidimos aumentar la cantidad de iteraciones a 10 pero manteniendo la cantidad de capas y neuronas. D√°ndonos como resultado un porcentaje de 99% de precisi√≥n

8.  Pero a√∫n no logramos verificar si el overfitting es por la cantidad de √©pocas o por cantidad de neuronas. As√≠ que decidimos seguir aumentando las vueltas pero ahora a 20. Demostrando que luego de la s√©ptima iteraci√≥n no hay una diferencia significativa en la precisi√≥n. Por lo que demasiadas vueltas dan overfitting

9.  As√≠ que para intentar nivelar probamos con 2 capas ocultas de 50 neuronas cada una, d√°ndonos un resultado similar a las anteriores, donde pareciera verse un overfitting en la cantidad de iteraciones, sobre todo despu√©s de la 7.¬™ iteraci√≥n.

10. As√≠ que decidimos aumentar la cantidad de neuronas, pero manteniendo las capas ocultas y ha mejorado desde un inicio el porcentaje de precisi√≥n. De momento pareciera que con un mayor n√∫mero de neuronas se obtiene un mejor resultado que con un mayor n√∫mero de capas ocultas.

11. Verificando la hip√≥tesis anterior, deber√≠amos esperar que al aumentar el n√∫mero de capas, pero disminuyendo el n√∫mero de neuronas, la precisi√≥n de la red sea menor y efectivamente resulta cierto.

12. As√≠ que para verificar a√∫n m√°s la hip√≥tesis que pareciera ser cierta, a√±adimos el doble de capas ocultas. Dejando un total de 8 resultandos en que la precisi√≥n de la red es menor que la anterior.

13. Y para salir de todas las dudas, hicimos la prueba final que consisti√≥ en tener solo dos capas ocultas pero con 200 neuronas cada una. D√°ndonos como resultado un porcentaje de precisi√≥n de 99% que es el mejor resultado hasta el momento.

### _Como conclusi√≥n final pareciera ser evidente que es mejor tener una buena cantidad de neuronas por capas, que tener muchas capas y pocas neuronas._

## üõ†Ô∏è Herramientas & Librerias

- [Tensorflow](https://www.tensorflow.org/) - Create production-grade machine learning models with TensorFlow
- [Pandas](https://pandas.pydata.org/) - Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,
built on top of the Python programming language.
- [matplotlib](https://matplotlib.org/) - Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. Matplotlib makes easy things easy and hard things possible.
- [numpy](https://numpy.org/) - The fundamental package for scientific computing with Python

‚å®Ô∏è with ‚ù§Ô∏è by [@camjasaez](https://github.com/camjasaez)
